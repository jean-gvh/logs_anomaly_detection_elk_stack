{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Détection d'anomalies en temps réel sur les logs\n",
        "\n",
        "Ce notebook permet d'appliquer un modèle pré-entraîné de détection d'anomalies sur de nouveaux logs système. Il utilise :\n",
        "- Un modèle Isolation Forest pré-entraîné\n",
        "- Des encodeurs de caractéristiques sauvegardés\n",
        "- Un pipeline de prétraitement des données"
      ],
      "metadata": {
        "id": "xesL_JF7eo3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import des bibliothèques nécessaires"
      ],
      "metadata": {
        "id": "5XVkB8CNeo3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fonction de chargement et préparation des données\n",
        "\n",
        "Cette fonction :\n",
        "- Charge les données avec les types appropriés\n",
        "- Convertit les dates en timestamps\n",
        "- Optimise la mémoire en utilisant des types catégoriels"
      ],
      "metadata": {
        "id": "vK7D9sQPeo3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data(file_path):\n",
        "    \"\"\"\n",
        "    Charge et prépare les données pour la détection\n",
        "    \"\"\"\n",
        "    dtypes = {\n",
        "        'Hostname': 'category',\n",
        "        'Process': 'category',\n",
        "        'IdProcess': 'str',\n",
        "        'Message': 'str'\n",
        "    }\n",
        "    parse_dates = ['Date']\n",
        "\n",
        "    # Charger les données\n",
        "    df = pd.read_csv(file_path, dtype=dtypes, parse_dates=parse_dates)\n",
        "\n",
        "    # Convertir datetime en timestamp\n",
        "    df['Date'] = df['Date'].astype('int64') // 10**9\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion du modèle au format .joblib\n",
        "\n",
        "* Cette fonction convertie le modèle en .plk au format .joblib."
      ],
      "metadata": {
        "id": "EIP5w1O-fTIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from joblib import dump\n",
        "\n",
        "# Charger le modèle au format .pkl\n",
        "with open(\"model_isolation_forest.pkl\", \"rb\") as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "# Sauvegarder le modèle au format joblib\n",
        "dump(model, \"isolation_forest_model.joblib\")\n",
        "\n",
        "print(\"Modèle converti et sauvegardé au format joblib avec succès.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpIEjiVXfdvJ",
        "outputId": "1b48320c-4f10-4a49-f9f7-1f255118fd45"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle converti et sauvegardé au format joblib avec succès.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fonction de détection des anomalies\n",
        "\n",
        "Cette fonction :\n",
        "- Charge le modèle et les encodeurs pré-entraînés\n",
        "- Encode les nouvelles données de la même manière que les données d'entraînement\n",
        "- Applique le modèle pour détecter les anomalies"
      ],
      "metadata": {
        "id": "LEndI43Weo3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomalies(df, model_path='isolation_forest_model.joblib', encoders_path='feature_encoders.joblib'):\n",
        "    \"\"\"\n",
        "    Detect anomalies in a new dataset\n",
        "    \"\"\"\n",
        "    # Load the model\n",
        "    saved_model = joblib.load(model_path)\n",
        "    if isinstance(saved_model, dict):\n",
        "        model = saved_model.get('model')\n",
        "        scaler = saved_model.get('scaler')\n",
        "    else:\n",
        "        model = saved_model\n",
        "        scaler = None  # Scaler may not exist in this case\n",
        "\n",
        "    # Load the encoders\n",
        "    encoders = joblib.load(encoders_path)\n",
        "\n",
        "    # Encode categorical features\n",
        "    df_encoded = df.copy()\n",
        "    for column, encoder in encoders.items():\n",
        "        df_encoded[column] = df_encoded[column].map(\n",
        "            lambda x: -1 if x not in encoder.classes_ else encoder.transform([x])[0]\n",
        "        )\n",
        "\n",
        "    # Normalize data if scaler exists\n",
        "    if scaler:\n",
        "        X_scaled = scaler.transform(df_encoded)\n",
        "    else:\n",
        "        X_scaled = df_encoded  # Use raw data if no scaler is available\n",
        "\n",
        "    # Predict anomalies\n",
        "    predictions = model.predict(X_scaled)\n",
        "\n",
        "    # Convert predictions (-1: anomaly, 1: normal)\n",
        "    anomalies = np.where(predictions == -1, 1, 0)\n",
        "\n",
        "    # Add predictions to the original DataFrame\n",
        "    df['anomaly'] = anomalies\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "detect_anomalies"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = joblib.load(\"isolation_forest_model.joblib\")\n",
        "print(saved_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9K8qA5cgd3f",
        "outputId": "ec057ccd-81ab-4825-df38-1917139943aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date_2024-12-11T16:37:38.628767+01:00'\n",
            " 'Date_2024-12-11T16:37:38.629082+01:00'\n",
            " 'Date_2024-12-11T16:37:38.630478+01:00' ...\n",
            " 'Message_xdg-desktop-portal-gnome.service: Main process exited, code=exited, status=1/FAILURE'\n",
            " \"Message_xdg-desktop-portal-gtk.service: Failed with result 'exit-code'.\"\n",
            " 'Message_xdg-desktop-portal-gtk.service: Main process exited, code=exited, status=1/FAILURE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exécution de la détection\n",
        "\n",
        "Cette section exécute le pipeline complet de détection d'anomalies sur les nouvelles données."
      ],
      "metadata": {
        "id": "_H8fdlFmeo3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin vers votre nouveau fichier CSV\n",
        "new_data_path = \"corrupted_logs.csv\"\n",
        "\n",
        "print(\"Chargement des données...\")\n",
        "df = load_and_prepare_data(new_data_path)\n",
        "\n",
        "print(\"Détection des anomalies...\")\n",
        "results = detect_anomalies(df)\n",
        "\n",
        "# Afficher un résumé\n",
        "n_anomalies = results['anomaly'].sum()\n",
        "print(f\"\\nNombre total d'anomalies détectées : {n_anomalies}\")\n",
        "print(f\"Pourcentage d'anomalies : {(n_anomalies / len(results)) * 100:.2f}%\")\n",
        "\n",
        "# Sauvegarder les résultats\n",
        "output_file = \"resultats_anomalies.csv\"\n",
        "results.to_csv(output_file, index=False)\n",
        "print(f\"\\nRésultats sauvegardés dans : {output_file}\")\n",
        "\n",
        "# Afficher quelques exemples d'anomalies\n",
        "print(\"\\nExemples d'anomalies détectées :\")\n",
        "print(results[results['anomaly'] == 1].head())"
      ],
      "metadata": {
        "id": "main_execution",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afbead8-9073-4be2-9abc-eaef130818ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement des données...\n",
            "Détection des anomalies...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Analyse des résultats\n",
        "\n",
        "Pour approfondir l'analyse des anomalies détectées, vous pouvez :\n",
        "1. Examiner la distribution temporelle des anomalies\n",
        "2. Analyser les patterns par host ou par processus\n",
        "3. Identifier les messages les plus fréquents dans les anomalies\n",
        "\n",
        "Voici quelques visualisations utiles :"
      ],
      "metadata": {
        "id": "SgIoT7NAeo3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution des anomalies par host\n",
        "anomalies_by_host = results[results['anomaly'] == 1]['Hostname'].value_counts()\n",
        "print(\"Distribution des anomalies par host :\")\n",
        "print(anomalies_by_host.head())\n",
        "\n",
        "# Distribution des anomalies par processus\n",
        "anomalies_by_process = results[results['anomaly'] == 1]['Process'].value_counts()\n",
        "print(\"\\nDistribution des anomalies par processus :\")\n",
        "print(anomalies_by_process.head())"
      ],
      "metadata": {
        "id": "analysis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}